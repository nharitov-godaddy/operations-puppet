<%#- SPDX-License-Identifier: Apache-2.0 -%>
# NOTE: This file is managed by Puppet.

# Default system properties included when running spark-submit.
# This is useful for setting default environmental settings.

# Example:
# spark.master                     spark://master:7077
# spark.eventLog.enabled           true
# spark.eventLog.dir               hdfs://namenode:8021/directory
# spark.serializer                 org.apache.spark.serializer.KryoSerializer
# spark.driver.memory              5g
# spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"

# Dynamic allocation allows Spark to dynamically scale the cluster resources
# allocated for an application based on the workload. Only available in YARN mode.
# More info: https://spark.apache.org/docs/2.1.2/configuration.html#dynamic-allocation
spark.dynamicAllocation.enabled                     true
spark.shuffle.service.enabled                       true
spark.dynamicAllocation.executorIdleTimeout         60s
spark.dynamicAllocation.cachedExecutorIdleTimeout   3600s
spark.shuffle.io.maxRetries                         10
spark.shuffle.io.retryWait                          10s
<% if @yarn_use_multi_spark_shufflers and @default_shuffler_version != '3.1' -%>
# The following two settings are only set if the cluster is configured
# to have multiple yarn shufflers available and the version is greater than 3.1
spark.shuffle.service.name = spark_shuffle_<%= @default_shuffler_version.gsub('.','_') %>
spark.shuffle.service.port = <%= @default_shuffler_port %>
<% end -%>
<% if @executor_env_ld_lib_path -%>
spark.executorEnv.LD_LIBRARY_PATH                   <%= @executor_env_ld_lib_path %>
<% end -%>
<% if @hive_enabled -%>
spark.sql.catalogImplementation                     hive
<% end -%>
<% if @hive_enabled and @iceberg_enabled -%>
# This setting adds support for Iceberg SQL extensions like CALL for stored procedures or ALTER TABLE ... WRITE ORDERED BY.
# See https://iceberg.apache.org/docs/1.2.1/spark-configuration/#sql-extensions
spark.sql.extensions                                org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
# These 'spark.sql.catalog.spark_catalog*' settings add support for Iceberg to the default Spark Catalog by wrapping it.
# See https://iceberg.apache.org/docs/1.2.1/spark-configuration/#replacing-the-session-catalog
spark.sql.catalog.spark_catalog                     org.apache.iceberg.spark.SparkSessionCatalog
spark.sql.catalog.spark_catalog.type                hive
<% end -%>
<% if @driver_port -%>
spark.driver.port                                   <%= @driver_port %>
<% end -%>
<% if @port_max_retries -%>
spark.port.maxRetries                               <%= @port_max_retries %>
<% end -%>
<% if @ui_port -%>
spark.ui.port                                       <%= @ui_port %>
<% end -%>
<% if @local_dir -%>
spark.local.dir                                     <%= @local_dir %>
<% end -%>
<% if @driver_blockmanager_port -%>
spark.driver.blockManager.port                      <%= @driver_blockmanager_port %>
<% end -%>
<% if @sql_files_max_partition_bytes -%>
spark.sql.files.maxPartitionBytes                   <%= @sql_files_max_partition_bytes %>
<% end -%>
spark.sql.warehouse.dir                             hdfs:///user/hive/warehouse
spark.yarn.archive                                  hdfs:///user/spark/share/lib/spark-<%= @spark_version %>-assembly.jar

# JVMs should use system proxy settings.
# The system proxy settings are configured via the env vars http_proxy, https_proxy, and no_proxy.
spark.driver.defaultJavaOptions                     -Djava.net.useSystemProxies=True
spark.executor.defaultJavaOptions                   -Djava.net.useSystemProxies=True

<% @extra_settings.sort.each do |key, value| -%>
<%= key %>   <%= value %>
<% end -%>

<% if @encryption_enabled -%>
spark.authenticate                                  true
# Spark IO encryption settings are not enabled (but listed anyway)
# since in some use cases (like Refine) they caused exceptions like
# 'java.io.IOException: Stream is corrupted' when shuffle files were
# compressed with lz4.
# spark.io.encryption.enabled                         true
# spark.io.encryption.keySizeBits                     256
# spark.io.encryption.keygen.algorithm                HmacSHA256
spark.network.crypto.enabled                        true
spark.network.crypto.keyFactoryAlgorithm            PBKDF2WithHmacSHA256
spark.network.crypto.keyLength                      256
spark.network.crypto.saslFallback                   false
<% end -%>

# Ensure that Python requests lib always use system CA certificates.
spark.yarn.appMasterEnv.REQUESTS_CA_BUNDLE          /etc/ssl/certs/ca-certificates.crt
spark.executorEnv.REQUESTS_CA_BUNDLE                /etc/ssl/certs/ca-certificates.crt
