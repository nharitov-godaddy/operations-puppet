<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
<!-- This file is maintained by puppet! -->
<!-- modules/dumps/files/web/html/mediawiki_history_readme.html -->
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Analytics: MediaWiki History</title>
    <link rel="stylesheet" type="text/css" href="/dumps.css" />
</head>
<body>
    <div id="globalWrapper">
        <div id="content">
            <h1>Analytics Datasets: MediaWiki History</h1>

            <h3>Contents</h3>
            <p>
                This data set contains a historical record of revision, user and page events of Wikimedia wikis since 2001.
                The data is denormalized, meaning that all events for user, page and revision are stored in the same schema.
                This leads to some fields being always null for some events (for instance fields about page are null in events about user).
                Events about users and pages have been processed to rebuild an as coherent as possible history in term of user-renames
                and page-moves (see <a href="https://wikitech.wikimedia.org/wiki/Analytics/Systems/Cluster/Page_and_user_history_reconstruction">
                the page and user history reconstruction wikitech page</a>).  Also, some precomputed fields have been added to facilitate analyses,
                such as edit-count per user and per page, reverting and reverted revisions and more. For further details visit
                <a href="https://wikitech.wikimedia.org/wiki/Analytics/Data_Lake/Edits/Mediawiki_history_dumps#Technical_Documentation">the MediaWiki history dumps dataset wikitech page</a>, containing the schema and links to some code examples.
            </p>

            <h3>Updates</h3>
            <p>
                The updates for this data set are monthly, around the end of the month's first week.
                Each update contains a full dump since 2001 (the beginning of MediaWiki-time) up to the current month.
                The reason for this particularity is the underlying data, the MediaWiki databases.
                Every time a user gets renamed, a revision reverted, a page moved, etc. the
                existing related records in the logging table are updated accordingly.
                So an event triggered today may change the state of that table 10 years ago.
                And it turns out the logging table is the base of the MediaWiki history reconstruction process.
                Thus, note that incremental downloads of these dumps may generate inconsistent data.
                Consider using <a href="https://wikitech.wikimedia.org/wiki/Event_Platform/EventStreams">EventStreams</a>
                for real time updates on MediaWiki changes
                (<a href="https://stream.wikimedia.org/?doc#/Streams">API docs</a>).
            </p>

            <h3>Versioning</h3>
            <p>
                Each update receives the name of the last featured month, in YYYY-MM format.
                For example if the dump spans from 2001 to August 2019 (included), it will be named 2019-08
                even if it will be released on the first days of September 2019.
                There is a folder for each available month at the root of the download URL, and for storage
                reasons only the last two versions are available. This shouldn't be problematic as every
                version contains the whole historical dataset.
            </p>

            <h3>Partitioning</h3>
            <p>
                The data is organized by wiki and time range. This way it can be
                downloaded for a single wiki (or set of wikis). The time split is
                necessary because of file size reasons. There are 3 different time range splits:
                monthly, yearly and all-time. Very big wikis are partitioned monthly, while
                medium wikis are partitioned yearly, and small wikis are dumped in one
                single file. This way we ensure that files are not larger than ~2GB,
                and at the same time we prevent generating a very large number of files.
                <ul>
                    <li>Wikis partitioned monthly: wikidatawiki, commonswiki, enwiki.</li>
                    <li>
                        Wikis partitioned yearly: dewiki, frwiki, eswiki, itwiki, ruwiki, jawiki, viwiki,
                        zhwiki, ptwiki, enwiktionary, plwiki, nlwiki, svwiki, metawiki, arwiki, shwiki,
                        cebwiki, mgwiktionary, fawiki, frwiktionary, ukwiki, hewiki, kowiki, srwiki, trwiki,
                        loginwiki, huwiki, cawiki, nowiki, mediawikiwiki, fiwiki, cswiki, idwiki, rowiki,
                        enwikisource, frwikisource, ruwiktionary, dawiki, bgwiki, incubatorwiki, enwikinews,
                        specieswiki, thwiki.
                    </li>
                    <li>Wikis in one single file: all the others.</li>
                </ul>
            </p>

            <h3>File format</h3>
            <p>
                The file format is tab-separated-value (TSV) instead of JSON in order to reduce the file sizes
                (JSON repeats field names for every record).
                Even if MediaWiki history data is pretty flat, it has some fields that are arrays of strings.
                The encoding of such arrays is the following:
                <span class="code">array(&lt;value1&gt;,&lt;value2&gt;,...,&lt;valueN&gt;)</span>.
                The compression algorithm is Bzip2, for it being widely used, free software,
                and having a high compression rate. Note that with Bzip2, you can concatenate
                several compressed files and treat them as a single Bzip2 file.
            </p>

            <h3>Directory structure</h3>
            <p>
                When choosing a file (or set of files) to download, the URL should look like this:<br/>
                <span class="code">/&lt;version&gt;/&lt;wiki&gt;/&lt;version&gt;.&lt;wiki&gt;.&lt;time range&gt;.tsv.bz2</span><br/>
                Where version is the YYYY-MM formated snapshot i.e. 2019-12; &lt;wiki&gt; is the wiki database name, i.e. enwiki or commonswiki;
                and &lt;time_range&gt; is either YYYY-MM for big wikis, YYYY for medium wikis, or all-time for the rest (see partitionning above).
                Examples of dump files:
                <ul>
                    <li class="code">/2019-12/wikidatawiki/2019-12.wikidatawiki.2019-05.tsv.bz2</li>
                    <li class="code">/2019-12/ptwiki/2019-12.ptwiki.2018.tsv.bz2</li>
                    <li class="code">/2019-12/cawikinews/2019-12.cawikinews.all-time.tsv.bz2</li>
                </ul>
            </p>

            <h2><a href="/other/mediawiki_history">Download MediaWiki History Data</a></h2>

            <p>
                If you're interested in how this data set is generated, have a look at the following articles:
                <ul>
                    <li><a href="https://wikitech.wikimedia.org/wiki/Analytics/Systems/Cluster/Edit_data_loading">
                        Loading source data from MediaWiki databases.
                    </a></li>
                    <li><a href="https://wikitech.wikimedia.org/wiki/Analytics/Systems/Cluster/Page_and_user_history_reconstruction">
                        Reconstructing page and user history.
                    </a></li>
                    <li><a href="https://wikitech.wikimedia.org/wiki/Analytics/Systems/Cluster/Revision_augmentation_and_denormalization">
                        Augmenting and denormalizing revision data.
                    </a></li>
                </ul>
            </p>

            <h4>Back to all <a href="/other/analytics">Analytics Datasets</a></h4>
            <div class="visualClear"></div>
            <hr/>
            <p>
                <strong>All Analytics datasets are available under the <a href = "https://creativecommons.org/publicdomain/zero/1.0/" target = "_blank">Creative Commons CC0 dedication</a>.</strong>
            </p>
        </div>
    </div>
</body>
</html>
