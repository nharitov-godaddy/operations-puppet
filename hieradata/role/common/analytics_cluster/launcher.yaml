# Notify the Data Engineering team about services on these hosts
contactgroups: 'admins,analytics'

nagios_group: analytics_eqiad
cluster: analytics
profile::admin::groups:
  - analytics-admins
  # https://phabricator.wikimedia.org/T326827
  - analytics-platform-eng-admins

profile::hadoop::common::hadoop_cluster_name: 'analytics-hadoop'

profile::hive::client::hive_service_name: 'analytics-hive'

profile::oozie::client::oozie_service: 'analytics-oozie'

# Set the hive-site.xml file with group ownership 'analytics' so systemd timers
# can read the file.
profile::hive::client::config_files_group_ownership: 'analytics'

profile::analytics::cluster::hdfs_mount::monitoring_user: 'analytics'

profile::analytics::refinery::job::project_namespace_map::http_proxy: 'http://webproxy.eqiad.wmnet:8080'

profile::analytics::refinery::job::data_purge::public_druid_host: 'druid1004.eqiad.wmnet'

profile::statistics::base::enable_stat_host_addons: false
profile::statistics::base::mysql_credentials_group: 'analytics'

profile::debdeploy::client::exclude_mounts:
  - /mnt/hdfs

profile::kerberos::keytabs::keytabs_metadata:
  - role: 'analytics'
    owner: 'analytics'
    group: 'analytics'
    filename: 'analytics.keytab'
  - role: 'hadoop'
    owner: 'hdfs'
    group: 'hdfs'
    filename: 'hdfs.keytab'
    parent_dir_grp: 'hadoop'
  # https://phabricator.wikimedia.org/T326827
  - role: 'analytics-platform-eng'
    owner: 'analytics-platform-eng'
    group: 'analytics-platform-eng'
    filename: 'analytics-platform-eng.keytab'

# Context https://phabricator.wikimedia.org/T278353#6976509
profile::kerberos::client::dns_canonicalize_hostname: false

profile::java::java_packages:
  - version: '8'
    variant: 'jdk'
profile::java::extra_args:
  JAVA_TOOL_OPTIONS: "-Dfile.encoding=UTF-8"

profile::analytics::refinery::ensure_hdfs_dirs: true

profile::analytics::refinery::job::hdfs_cleaner::ensure_timer: present

# This parameter can be used to disable ingestion via gobblin temporarily,
# such as when Hadoop maintenance work is required
profile::analytics::refinery::job::gobblin::ensure_timers: present

# Versioned gobblin-wmf shaded to use for gobblin ingestion jobs.
profile::analytics::refinery::job::gobblin::gobblin_jar_file: /srv/deployment/analytics/refinery/artifacts/org/wikimedia/gobblin-wmf/gobblin-wmf-core-1.0.1-jar-with-dependencies.jar

profile::airflow::airflow_version: '2.5.1-py3.10-20230228'
profile::airflow::database_host_default: an-db1001.eqiad.wmnet

# Set up airflow instances.
profile::airflow::instances:
  # airflow@analytics instance.
  analytics:
    ferm_srange: $ANALYTICS_NETWORKS
    # Since we set security: kerberos a keytab must be deployed for the service_user.
    service_user: analytics
    service_group: analytics
    monitoring_enabled: true
    connections:
      analytics-hive:
        conn_type: hive_metastore
        host: analytics-hive.eqiad.wmnet
        port: 9083
        extra_dejson: {'authMechanism': 'GSSAPI'}
      datahub_kafka_jumbo:
        conn_type: datahub_kafka
        host: kafka-jumbo1001.eqiad.wmnet:9092
        extra_dejson: {"connection": {"schema_registry_url": "http://karapace1001.eqiad.wmnet:8081"}}

profile::contacts::role_contacts: ['Data Engineering']

profile::base::certificates::include_bundle_jks: true

profile::analytics::conda_analytics::remove_conda_env_pkgs_dir: false
